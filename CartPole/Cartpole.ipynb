{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4 (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "291dLoKw5iyh",
        "colab_type": "code",
        "outputId": "f0c23d64-aa5c-4ffe-d784-db50856a1064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRbPWX73X8SQ",
        "colab_type": "text"
      },
      "source": [
        "**In this file i have shown the comparison between Diuble DDQN and DQN. **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFrfwgJs5u6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"CartPole-v0\"\n",
        "env = gym.make(env_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7t-hd4WBukd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#main aim for ddqn is avoid selecting the maximum value too early \n",
        "# ---> ddqn helps us to select which actions and state to take which are present in local and arget neural network\n",
        "class QNetwork():\n",
        "    def __init__(self, state_dim, action_size, tau=0.01):\n",
        "        tf.compat.v1.reset_default_graph()\n",
        "        self.state_in = tf.placeholder(tf.float32, shape=[None, *state_dim])\n",
        "        self.action_in = tf.placeholder(tf.int32, shape=[None])\n",
        "        self.q_target_in = tf.placeholder(tf.float32, shape=[None])\n",
        "        action_one_hot = tf.one_hot(self.action_in, depth=action_size)\n",
        "        \n",
        "        self.q_state_local = self.build_model(action_size, \"local\") #calling function for local q-state\n",
        "        self.q_state_target = self.build_model(action_size, \"target\") #calling function for target q-state\n",
        "        \n",
        "        self.q_state_action = tf.reduce_sum(tf.multiply(self.q_state_local, action_one_hot), axis=1)\n",
        "        self.loss = tf.reduce_mean(tf.square(self.q_state_action - self.q_target_in))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.loss) #optimizer is adam optimizer; iteratively updating network weights, in training data\n",
        "        #accesing the network through tensorflow function\n",
        "        self.local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"local\")\n",
        "        self.target_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"target\") #specifying the keys for target network\n",
        "        self.updater = tf.group([tf.assign(t, t + tau*(l-t)) for t,l in zip(self.target_vars, self.local_vars)]) #update operation with group function\n",
        "           \n",
        "    #defining the function if the target value is true(boolean), else we can use local network.        \n",
        "    def get_q_state(self, session, state, use_target=False):\n",
        "        q_state_op = self.q_state_target if use_target else self.q_state_local\n",
        "        q_state = session.run(q_state_op, feed_dict={self.state_in: state})\n",
        "        return q_state       \n",
        "\n",
        "     #dense layer, of target network, the layers are same as neural network, but params are different\n",
        "    def build_model(self, action_size, scope):\n",
        "        with tf.variable_scope(scope): #seprating the scope\n",
        "            hidden1 = tf.layers.dense(self.state_in, 100, activation=tf.nn.relu)\n",
        "            q_state = tf.layers.dense(hidden1, action_size, activation=None)\n",
        "            return q_state\n",
        "               \n",
        "    def update_model(self, session, state, action, q_target):\n",
        "        feed = {self.state_in: state, self.action_in: action, self.q_target_in: q_target}\n",
        "        session.run([self.optimizer, self.updater], feed_dict=feed)\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJyLyX4zB8aM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Buffering():\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)   \n",
        "               \n",
        "    def __init__(self, maxlen):\n",
        "        self.buffer = deque(maxlen=maxlen)\n",
        "             \n",
        "    def sample(self, batch_size):\n",
        "        sample_size = min(len(self.buffer), batch_size)\n",
        "        samples = random.choices(self.buffer, k=sample_size)\n",
        "        return map(list, zip(*samples))\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG6qbavJCD1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class DoubleDQNAgent():\n",
        "    def __init__(self, env):\n",
        "        self.state_dim = env.observation_space.shape\n",
        "        self.action_size = env.action_space.n\n",
        "        self.q_network = QNetwork(self.state_dim, self.action_size)\n",
        "        self.replay_buffer = Buffering(maxlen=10000)\n",
        "        self.gamma = 0.97\n",
        "        self.eps = 1.0\n",
        "        \n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "    def train(self, state, action, next_state, reward, done, use_DDQN=True): #we have to define our q-targets and specify whether we are using ddqn or not\n",
        "        self.replay_buffer.add((state, action, next_state, reward, done))\n",
        "        states, actions, next_states, rewards, dones = self.replay_buffer.sample(50)  \n",
        "        next_actions = np.argmax(self.q_network.get_q_state(self.sess, next_states, use_target=False), axis=1) #we get the arguments of the target nets from the local nets that we are using\n",
        "        q_next_states = self.q_network.get_q_state(self.sess, next_states, use_target=use_DDQN) #over here you can turn on and off, whether you want to use ddqn or not\n",
        "        q_next_states[dones] = np.zeros([self.action_size])\n",
        "        q_next_states_next_actions = q_next_states[np.arange(next_actions.shape[0]), next_actions] #next action and next state of the target net\n",
        "        q_targets = rewards + self.gamma * q_next_states_next_actions\n",
        "        self.q_network.update_model(self.sess, states, actions, q_targets)\n",
        "        \n",
        "        if done: self.eps = max(0.1, 0.99*self.eps)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        q_state = self.q_network.get_q_state(self.sess, [state])\n",
        "        action_greedy = np.argmax(q_state)\n",
        "        action_random = np.random.randint(self.action_size)\n",
        "        action = action_random if random.random() < self.eps else action_greedy\n",
        "        return action\n",
        "    \n",
        "    def __del__(self):\n",
        "        self.sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uAMlqWTCx4m",
        "colab_type": "code",
        "outputId": "d6493d22-ae2b-4bbe-fae2-b73186f7c094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 978
        }
      },
      "source": [
        "num_runs = 50\n",
        "run_rewards = []\n",
        "for n in range(num_runs):\n",
        "    print(\"Run {}\".format(n))\n",
        "    ep_rewards = []\n",
        "    agent = None #since each agent creates a tensorflow session, we need define it none, it would delete all the agents\n",
        "    agent = DoubleDQNAgent(env)\n",
        "    num_episodes = 500\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.get_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            agent.train(state, action, next_state, reward, done, use_DDQN=(n%2==0)) #over here we are running all the ddqn on every even run, to show the difference between dqn and ddqn\n",
        "            #env.render()\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "        ep_rewards.append(total_reward) #saving all the rewards to a list          \n",
        "    run_rewards.append(ep_rewards)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run 0\n",
            "WARNING:tensorflow:From <ipython-input-4-83ab36c11408>:29: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "Run 1\n",
            "Run 2\n",
            "Run 3\n",
            "Run 4\n",
            "Run 5\n",
            "Run 6\n",
            "Run 7\n",
            "Run 8\n",
            "Run 9\n",
            "Run 10\n",
            "Run 11\n",
            "Run 12\n",
            "Run 13\n",
            "Run 14\n",
            "Run 15\n",
            "Run 16\n",
            "Run 17\n",
            "Run 18\n",
            "Run 19\n",
            "Run 20\n",
            "Run 21\n",
            "Run 22\n",
            "Run 23\n",
            "Run 24\n",
            "Run 25\n",
            "Run 26\n",
            "Run 27\n",
            "Run 28\n",
            "Run 29\n",
            "Run 30\n",
            "Run 31\n",
            "Run 32\n",
            "Run 33\n",
            "Run 34\n",
            "Run 35\n",
            "Run 36\n",
            "Run 37\n",
            "Run 38\n",
            "Run 39\n",
            "Run 40\n",
            "Run 41\n",
            "Run 42\n",
            "Run 43\n",
            "Run 44\n",
            "Run 45\n",
            "Run 46\n",
            "Run 47\n",
            "Run 48\n",
            "Run 49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab_choYON8eg",
        "colab_type": "code",
        "outputId": "37fe9d39-5fe3-421f-b85e-5d01b1e72b52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "#visualiing the difference ddqn vs dqn in the performance\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "for n, ep_rewards in enumerate(run_rewards):\n",
        "    x = range(len(ep_rewards))\n",
        "    cumsum = np.cumsum(ep_rewards) #average rewards for every episodes\n",
        "    avgs = [cumsum[ep]/(ep+1) if ep<100 else (cumsum[ep]-cumsum[ep-200])/200 for ep in x]\n",
        "    col = \"r\" if (n%2==0) else \"b\"\n",
        "    plt.plot(x, avgs, color=col)\n",
        "    \n",
        "plt.title(\"DQN vs DDQN performance\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Last 200 episode average rewards\")\n",
        "plt.legend()\n",
        "#down in the graph we can see the differnce in the performance in ddqn and dqn, where ddqn are giving faster cumuliative rewards than dqn in fewer episodes"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4cf95793d94a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_rewards\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcumsum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_rewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#average rewards for every episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'run_rewards' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix0vSfJ1N-vE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}